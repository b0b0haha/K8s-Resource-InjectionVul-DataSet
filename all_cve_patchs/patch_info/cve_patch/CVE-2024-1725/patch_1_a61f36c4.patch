From a61f36c42700f54352919318ed806d1ae2d716f4 Mon Sep 17 00:00:00 2001
From: Michael Henriksen <mhenriks@redhat.com>
Date: Fri, 8 Mar 2024 10:09:04 -0500
Subject: [PATCH] Address CVE-2024-1725:  Restrict access to infrastructure
 PVCs by requiring matching infraClusterLabels on tenant PVCs

The CVE describes how an attacker may create a PV/PVC in a guest cluster to access any PVC in the infra cluster namespace.
The infra clusters may belong to other guest clusters or have been created out of band from the kubevirt-csi driver.

This PR addresses the issue by:

1.  infraClusterLabels are required (but is up to admin to make sure they are unique per tenant)
2.  guest may only access infra PVCs with matching labels
3.  guest can only access PVCs with specific prefix (default is "pvc-")

Shoutout to awels who actually implemented this based on input from davidvossel.

Co-authored-by: Alexander Wels <awels@redhat.com>
Co-authored-by: Michael Henriksen <mhenriks@redhat.com>
Signed-off-by: Michael Henriksen <mhenriks@redhat.com>
---
 Makefile                                      |   8 +-
 .../kubevirt-csi-driver.go                    |  10 +-
 e2e/common_test.go                            |  10 ++
 e2e/create-pvc_test.go                        | 134 ++++++++++++++++++
 hack/cluster-sync.sh                          |   1 +
 pkg/kubevirt/client.go                        |  32 ++++-
 pkg/kubevirt/client_test.go                   | 129 +++++++++++++----
 pkg/service/controller.go                     |   5 +
 8 files changed, 296 insertions(+), 33 deletions(-)

diff --git a/Makefile b/Makefile
index d3aaee75..a3c354b5 100644
--- a/Makefile
+++ b/Makefile
@@ -46,7 +46,11 @@ include $(addprefix ./vendor/github.com/openshift/build-machinery-go/make/, \
 # You can list all codegen related variables by:
 #   $ make -n --print-data-base | grep ^CODEGEN
 .PHONY: image-build
-image-build: generate
+# let's disable generate for for now
+# it updates libs and I think it is better to do that manually
+# especially when changes will be backported
+#image-build: generate
+image-build:
 	source ./hack/cri-bin.sh && \
 	$$CRI_BIN build -t $(IMAGE_REF) --build-arg git_sha=$(SHA) .
 
@@ -116,4 +120,4 @@ sanity-test:
 
 .PHONY: generate
 generate:
-	./hack/generate_clients.sh
\ No newline at end of file
+	./hack/generate_clients.sh
diff --git a/cmd/kubevirt-csi-driver/kubevirt-csi-driver.go b/cmd/kubevirt-csi-driver/kubevirt-csi-driver.go
index 66e8d909..f7864d9a 100644
--- a/cmd/kubevirt-csi-driver/kubevirt-csi-driver.go
+++ b/cmd/kubevirt-csi-driver/kubevirt-csi-driver.go
@@ -25,6 +25,7 @@ var (
 	infraClusterNamespace  = flag.String("infra-cluster-namespace", "", "The infra-cluster namespace")
 	infraClusterKubeconfig = flag.String("infra-cluster-kubeconfig", "", "the infra-cluster kubeconfig file. If not set, defaults to in cluster config.")
 	infraClusterLabels     = flag.String("infra-cluster-labels", "", "The infra-cluster labels to use when creating resources in infra cluster. 'name=value' fields separated by a comma")
+	volumePrefix           = flag.String("volume-prefix", "pvc", "The prefix expected for persistent volumes")
 	// infraStorageClassEnforcement = flag.String("infra-storage-class-enforcement", "", "A string encoded yaml that represents the policy of enforcing which infra storage classes are allowed in persistentVolume of type kubevirt")
 	infraStorageClassEnforcement = os.Getenv("INFRA_STORAGE_CLASS_ENFORCEMENT")
 
@@ -54,6 +55,13 @@ func handle() {
 	}
 	klog.V(2).Infof("Driver vendor %v %v", service.VendorName, service.VendorVersion)
 
+	if (infraClusterLabels == nil || *infraClusterLabels == "") && !*runNodeService {
+		klog.Fatal("infra-cluster-labels must be set")
+	}
+	if volumePrefix == nil || *volumePrefix == "" {
+		klog.Fatal("volume-prefix must be set")
+	}
+
 	inClusterConfig, err := rest.InClusterConfig()
 	if err != nil {
 		klog.Fatalf("Failed to build in cluster config: %v", err)
@@ -86,7 +94,7 @@ func handle() {
 	infraClusterLabelsMap := parseLabels()
 	storageClassEnforcement := configureStorageClassEnforcement(infraStorageClassEnforcement)
 
-	virtClient, err := kubevirt.NewClient(infraRestConfig, infraClusterLabelsMap, storageClassEnforcement)
+	virtClient, err := kubevirt.NewClient(infraRestConfig, infraClusterLabelsMap, storageClassEnforcement, *volumePrefix)
 	if err != nil {
 		klog.Fatal(err)
 	}
diff --git a/e2e/common_test.go b/e2e/common_test.go
index 5ad870d5..ce0ef11a 100644
--- a/e2e/common_test.go
+++ b/e2e/common_test.go
@@ -22,6 +22,7 @@ import (
 	"k8s.io/client-go/tools/clientcmd"
 	clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
 	"k8s.io/klog/v2"
+	cdicli "kubevirt.io/csi-driver/pkg/generated/containerized-data-importer/client-go/clientset/versioned"
 )
 
 // RunCmd function executes a command, and returns STDOUT and STDERR bytes
@@ -192,6 +193,15 @@ func generateInfraClient() (*kubernetes.Clientset, error) {
 	return kubernetes.NewForConfig(restConfig)
 }
 
+func generateInfraCdiClient() (*cdicli.Clientset, error) {
+	restConfig, err := generateInfraRestConfig()
+	if err != nil {
+		return nil, err
+	}
+
+	return cdicli.NewForConfig(restConfig)
+}
+
 func generateInfraSnapClient() (*snapcli.Clientset, error) {
 	restConfig, err := generateInfraRestConfig()
 	if err != nil {
diff --git a/e2e/create-pvc_test.go b/e2e/create-pvc_test.go
index f36b3469..9ab42ea7 100644
--- a/e2e/create-pvc_test.go
+++ b/e2e/create-pvc_test.go
@@ -10,18 +10,22 @@ import (
 
 	"k8s.io/apimachinery/pkg/api/errors"
 	v1 "k8s.io/client-go/kubernetes/typed/core/v1"
+	"k8s.io/klog/v2"
 
 	"k8s.io/client-go/tools/clientcmd"
+	cdicli "kubevirt.io/csi-driver/pkg/generated/containerized-data-importer/client-go/clientset/versioned"
 	kubecli "kubevirt.io/csi-driver/pkg/generated/kubevirt/client-go/clientset/versioned"
 
 	. "github.com/onsi/ginkgo/v2"
 	. "github.com/onsi/gomega"
+
 	"github.com/spf13/pflag"
 	k8sv1 "k8s.io/api/core/v1"
 	"k8s.io/apimachinery/pkg/api/resource"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/util/rand"
 	"k8s.io/client-go/kubernetes"
+	cdiv1 "kubevirt.io/containerized-data-importer-api/pkg/apis/core/v1beta1"
 )
 
 var virtClient *kubecli.Clientset
@@ -282,6 +286,136 @@ var _ = Describe("CreatePVC", func() {
 		Entry("Filesystem volume mode", k8sv1.PersistentVolumeFilesystem, attacherPodFs),
 		Entry("Block volume mode", k8sv1.PersistentVolumeBlock, attacherPodBlock),
 	)
+
+	Context("Should prevent access to volumes from infra cluster", func() {
+		var tenantPVC *k8sv1.PersistentVolumeClaim
+		var tenantPV *k8sv1.PersistentVolume
+		var infraDV *cdiv1.DataVolume
+		var infraCdiClient *cdicli.Clientset
+		BeforeEach(func() {
+			var err error
+			infraCdiClient, err = generateInfraCdiClient()
+			Expect(err).ToNot(HaveOccurred())
+		})
+
+		AfterEach(func() {
+			By("Cleaning up resources for test")
+			if tenantPVC != nil {
+				err := tenantClient.CoreV1().PersistentVolumeClaims(tenantPVC.Namespace).Delete(context.Background(), tenantPVC.Name, metav1.DeleteOptions{})
+				Expect(err).ToNot(HaveOccurred())
+				Eventually(func() bool {
+					_, err := tenantClient.CoreV1().PersistentVolumeClaims(tenantPVC.Namespace).Get(context.Background(), tenantPVC.Name, metav1.GetOptions{})
+					return errors.IsNotFound(err)
+				}, 1*time.Minute, 2*time.Second).Should(BeTrue(), "tenant pvc should disappear")
+				tenantPVC = nil
+			}
+			if tenantPV != nil {
+				err := tenantClient.CoreV1().PersistentVolumes().Delete(context.Background(), tenantPV.Name, metav1.DeleteOptions{})
+				Expect(err).ToNot(HaveOccurred())
+				// For some reason this takes about 2 minutes.
+				Eventually(func() bool {
+					_, err := tenantClient.CoreV1().PersistentVolumes().Get(context.Background(), tenantPV.Name, metav1.GetOptions{})
+					return errors.IsNotFound(err)
+				}, 3*time.Minute, 2*time.Second).Should(BeTrue(), "tenant pv should disappear")
+				tenantPV = nil
+			}
+			if infraDV != nil {
+				_, err := infraCdiClient.CdiV1beta1().DataVolumes(InfraClusterNamespace).Get(context.Background(), infraDV.Name, metav1.GetOptions{})
+				Expect(err).ToNot(HaveOccurred())
+				err = infraCdiClient.CdiV1beta1().DataVolumes(InfraClusterNamespace).Delete(context.Background(), infraDV.Name, metav1.DeleteOptions{})
+				Expect(err).ToNot(HaveOccurred())
+				infraDV = nil
+			}
+		})
+
+		It("should not be able to create a PV and access a volume from the infra cluster that is not labeled", func() {
+			infraDV = &cdiv1.DataVolume{
+				ObjectMeta: metav1.ObjectMeta{
+					Name:      "infra-pvc",
+					Namespace: InfraClusterNamespace,
+				},
+				Spec: cdiv1.DataVolumeSpec{
+					Source: &cdiv1.DataVolumeSource{
+						Blank: &cdiv1.DataVolumeBlankImage{},
+					},
+					Storage: &cdiv1.StorageSpec{
+						Resources: k8sv1.ResourceRequirements{
+							Requests: k8sv1.ResourceList{
+								k8sv1.ResourceStorage: resource.MustParse("1Gi"),
+							},
+						},
+					},
+				},
+			}
+			var err error
+			infraDV, err = infraCdiClient.CdiV1beta1().DataVolumes(InfraClusterNamespace).Create(context.Background(), infraDV, metav1.CreateOptions{})
+			Expect(err).ToNot(HaveOccurred())
+
+			By("Creating a specially crafted PV, attempt to access volume from infra cluster that should not be accessed")
+			tenantPV = &k8sv1.PersistentVolume{
+				ObjectMeta: metav1.ObjectMeta{
+					Name: "tenant-pv",
+				},
+				Spec: k8sv1.PersistentVolumeSpec{
+					AccessModes: []k8sv1.PersistentVolumeAccessMode{k8sv1.ReadWriteOnce},
+					Capacity:    k8sv1.ResourceList{k8sv1.ResourceStorage: resource.MustParse("1Gi")},
+					PersistentVolumeSource: k8sv1.PersistentVolumeSource{
+						CSI: &k8sv1.CSIPersistentVolumeSource{
+							Driver:       "csi.kubevirt.io",
+							VolumeHandle: infraDV.Name,
+							VolumeAttributes: map[string]string{
+								"bus":    "scsi",
+								"serial": "abcd",
+								"storage.kubernetes.io/csiProvisionerIdentity": "1708112628060-923-csi.kubevirt.io",
+							},
+							FSType: "ext4",
+						},
+					},
+					StorageClassName:              "kubevirt",
+					PersistentVolumeReclaimPolicy: k8sv1.PersistentVolumeReclaimDelete,
+				},
+			}
+			_, err = tenantClient.CoreV1().PersistentVolumes().Create(context.Background(), tenantPV, metav1.CreateOptions{})
+			Expect(err).ToNot(HaveOccurred())
+			tenantPVC = &k8sv1.PersistentVolumeClaim{
+				ObjectMeta: metav1.ObjectMeta{
+					Name: "tenant-pvc",
+				},
+				Spec: k8sv1.PersistentVolumeClaimSpec{
+					AccessModes: []k8sv1.PersistentVolumeAccessMode{k8sv1.ReadWriteOnce},
+					Resources: k8sv1.ResourceRequirements{
+						Requests: k8sv1.ResourceList{
+							k8sv1.ResourceStorage: resource.MustParse("1Gi"),
+						},
+					},
+					VolumeName: tenantPV.Name,
+				},
+			}
+			tenantPVC, err = tenantClient.CoreV1().PersistentVolumeClaims(namespace).Create(context.Background(), tenantPVC, metav1.CreateOptions{})
+			Expect(err).ToNot(HaveOccurred())
+			pod := writerPodFs(tenantPVC.Name)
+			By("Creating pod that attempts to use the specially crafted PVC")
+			pod, err = tenantClient.CoreV1().Pods(namespace).Create(context.Background(), pod, metav1.CreateOptions{})
+			Expect(err).ToNot(HaveOccurred())
+			defer deletePod(tenantClient.CoreV1(), namespace, pod.Name)
+
+			involvedObject := fmt.Sprintf("involvedObject.name=%s", pod.Name)
+			By("Waiting for error event to show up in pod event log")
+			Eventually(func() bool {
+				list, err := tenantClient.CoreV1().Events(namespace).List(context.Background(), metav1.ListOptions{
+					FieldSelector: involvedObject, TypeMeta: metav1.TypeMeta{Kind: "Pod"},
+				})
+				Expect(err).ToNot(HaveOccurred())
+				for _, event := range list.Items {
+					klog.Infof("Event: %s [%s]", event.Message, event.Reason)
+					if event.Reason == "FailedAttachVolume" && strings.Contains(event.Message, "invalid volume name") {
+						return true
+					}
+				}
+				return false
+			}, 30*time.Second, time.Second).Should(BeTrue(), "error event should show up in pod event log")
+		})
+	})
 })
 
 func writerPodFs(volumeName string) *k8sv1.Pod {
diff --git a/hack/cluster-sync.sh b/hack/cluster-sync.sh
index 28a21616..0033bdf2 100755
--- a/hack/cluster-sync.sh
+++ b/hack/cluster-sync.sh
@@ -8,6 +8,7 @@ INFRA_STORAGE_CLASS=${INFRA_STORAGE_CLASS:-rook-ceph-block}
 REGISTRY=${REGISTRY:-192.168.66.2:5000}
 TARGET_NAME=${TARGET_NAME:-kubevirt-csi-driver}
 TAG=${TAG:-latest}
+export INFRACLUSTER_LABELS=${INFRACLUSTER_LABELS:-"tenant-cluster=${TENANT_CLUSTER_NAMESPACE}"}
 
 function tenant::deploy_kubeconfig_secret() {
   TOKEN=$(_kubectl create token kubevirt-csi -n $TENANT_CLUSTER_NAMESPACE)
diff --git a/pkg/kubevirt/client.go b/pkg/kubevirt/client.go
index 9598c162..182004f8 100644
--- a/pkg/kubevirt/client.go
+++ b/pkg/kubevirt/client.go
@@ -5,6 +5,7 @@ import (
 	"encoding/json"
 	goerrors "errors"
 	"fmt"
+	"strings"
 	"time"
 
 	snapshotv1 "github.com/kubernetes-csi/external-snapshotter/client/v6/apis/volumesnapshot/v1"
@@ -62,10 +63,11 @@ type client struct {
 	restClient              *rest.RESTClient
 	storageClassEnforcement util.StorageClassEnforcement
 	infraLabelMap           map[string]string
+	volumePrefix            string
 }
 
 // NewClient New creates our client wrapper object for the actual kubeVirt and kubernetes clients we use.
-func NewClient(config *rest.Config, infraClusterLabelMap map[string]string, storageClassEnforcement util.StorageClassEnforcement) (Client, error) {
+func NewClient(config *rest.Config, infraClusterLabelMap map[string]string, storageClassEnforcement util.StorageClassEnforcement, prefix string) (Client, error) {
 	result := &client{}
 
 	Scheme := runtime.NewScheme()
@@ -108,6 +110,7 @@ func NewClient(config *rest.Config, infraClusterLabelMap map[string]string, stor
 	result.restClient = restClient
 	result.snapClient = snapClient
 	result.infraLabelMap = infraClusterLabelMap
+	result.volumePrefix = fmt.Sprintf("%s-", prefix)
 	result.storageClassEnforcement = storageClassEnforcement
 	return result, nil
 }
@@ -211,7 +214,11 @@ func (c *client) GetVirtualMachine(ctx context.Context, namespace, name string)
 
 // CreateDataVolume creates a new DataVolume under a namespace
 func (c *client) CreateDataVolume(ctx context.Context, namespace string, dataVolume *cdiv1.DataVolume) (*cdiv1.DataVolume, error) {
-	return c.cdiClient.CdiV1beta1().DataVolumes(namespace).Create(ctx, dataVolume, metav1.CreateOptions{})
+	if !strings.HasPrefix(dataVolume.GetName(), c.volumePrefix) {
+		return nil, ErrInvalidVolume
+	} else {
+		return c.cdiClient.CdiV1beta1().DataVolumes(namespace).Create(ctx, dataVolume, metav1.CreateOptions{})
+	}
 }
 
 // Ping performs a minimal request to the infra-cluster k8s api
@@ -222,11 +229,27 @@ func (c *client) Ping(ctx context.Context) error {
 
 // DeleteDataVolume deletes a DataVolume from a namespace by name
 func (c *client) DeleteDataVolume(ctx context.Context, namespace string, name string) error {
-	return c.cdiClient.CdiV1beta1().DataVolumes(namespace).Delete(ctx, name, metav1.DeleteOptions{})
+	if dv, err := c.GetDataVolume(ctx, namespace, name); errors.IsNotFound(err) {
+		return nil
+	} else if err != nil {
+		return err
+	} else if dv != nil {
+		return c.cdiClient.CdiV1beta1().DataVolumes(namespace).Delete(ctx, dv.Name, metav1.DeleteOptions{})
+	}
+	return nil
 }
 
 func (c *client) GetDataVolume(ctx context.Context, namespace string, name string) (*cdiv1.DataVolume, error) {
-	return c.cdiClient.CdiV1beta1().DataVolumes(namespace).Get(ctx, name, metav1.GetOptions{})
+	dv, err := c.cdiClient.CdiV1beta1().DataVolumes(namespace).Get(ctx, name, metav1.GetOptions{})
+	if err != nil {
+		return nil, err
+	}
+	if dv != nil {
+		if !containsLabels(dv.Labels, c.infraLabelMap) || !strings.HasPrefix(dv.GetName(), c.volumePrefix) {
+			return nil, ErrInvalidVolume
+		}
+	}
+	return dv, nil
 }
 
 func (c *client) CreateVolumeSnapshot(ctx context.Context, namespace, name, claimName, snapshotClassName string) (*snapshotv1.VolumeSnapshot, error) {
@@ -407,3 +430,4 @@ func (c *client) ListVolumeSnapshots(ctx context.Context, namespace string) (*sn
 }
 
 var ErrInvalidSnapshot = goerrors.New("invalid snapshot name")
+var ErrInvalidVolume = goerrors.New("invalid volume name")
diff --git a/pkg/kubevirt/client_test.go b/pkg/kubevirt/client_test.go
index e3436d65..20b07b9e 100644
--- a/pkg/kubevirt/client_test.go
+++ b/pkg/kubevirt/client_test.go
@@ -9,6 +9,7 @@ import (
 
 	k8sv1 "k8s.io/api/core/v1"
 	storagev1 "k8s.io/api/storage/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/runtime"
 	k8sfake "k8s.io/client-go/kubernetes/fake"
@@ -20,8 +21,8 @@ import (
 )
 
 const (
-	storageClassName             = "test-storage-class"
 	defaultStorageClassName      = "default-storage-class"
+	storageClassName             = "test-storage-class"
 	volumeSnapshotClassName      = "test-volume-snapshot-class"
 	provisioner                  = "test-provisioner"
 	nonMatchingProvisioner       = "non-matching-provisioner-snapshot-class"
@@ -29,9 +30,11 @@ const (
 	otherVolumeSnapshotClassName = "other-volume-snapshot-class"
 	testVolumeName               = "test-volume"
 	testVolumeNameNotAllowed     = "test-volume-not-allowed"
-	testClaimName                = "test-claim"
-	testClaimName2               = "test-claim2"
-	testClaimName3               = "test-claim3"
+	validDataVolume              = "pvc-valid-data-volume"
+	nolabelDataVolume            = "nolabel-data-volume"
+	testClaimName                = "pvc-valid-data-volume"
+	testClaimName2               = "pvc-valid-data-volume2"
+	testClaimName3               = "pvc-valid-data-volume3"
 	testNamespace                = "test-namespace"
 	unboundTestClaimName         = "unbound-test-claim"
 )
@@ -41,6 +44,63 @@ var _ = Describe("Client", func() {
 		c *client
 	)
 
+	Context("volumes", func() {
+		BeforeEach(func() {
+			// Setup code before each test
+			c = NewFakeClient()
+			c = NewFakeCdiClient(c, createValidDataVolume(), createNoLabelDataVolume(), createWrongPrefixDataVolume())
+		})
+
+		DescribeTable("GetDataVolume should return the right thing", func(volumeName string, expectedErr error) {
+			_, err := c.GetDataVolume(context.Background(), testNamespace, volumeName)
+			if expectedErr != nil {
+				Expect(err).To(Equal(expectedErr))
+			} else {
+				Expect(err).ToNot(HaveOccurred())
+			}
+		},
+			Entry("when the data volume exists", validDataVolume, nil),
+			Entry("when the data volume exists, but no labels", nolabelDataVolume, ErrInvalidVolume),
+			Entry("when the data volume exists, but no labels", testVolumeName, ErrInvalidVolume),
+		)
+
+		It("should return not exists if the data volume does not exist", func() {
+			_, err := c.GetDataVolume(context.Background(), testNamespace, "notexist")
+			Expect(err).To(HaveOccurred())
+			Expect(errors.IsNotFound(err)).To(BeTrue())
+		})
+
+		It("DeleteDataVolume should not delete volumes if the right prefix doesn't exist", func() {
+			err := c.DeleteDataVolume(context.Background(), testNamespace, testVolumeName)
+			Expect(err).To(HaveOccurred())
+			Expect(err).To(Equal(ErrInvalidVolume))
+		})
+
+		It("DeleteDataVolume return nil if volume doesn't exist", func() {
+			err := c.DeleteDataVolume(context.Background(), testNamespace, "notexist")
+			Expect(err).ToNot(HaveOccurred())
+		})
+
+		It("DeleteDataVolume should delete volumes if valid", func() {
+			err := c.DeleteDataVolume(context.Background(), testNamespace, validDataVolume)
+			Expect(err).ToNot(HaveOccurred())
+		})
+
+		It("Should create a volume if a valid volume is passed", func() {
+			dataVolume := createValidDataVolume()
+			dataVolume.Name = "pvc-test2"
+			_, err := c.CreateDataVolume(context.Background(), testNamespace, dataVolume)
+			Expect(err).ToNot(HaveOccurred())
+		})
+
+		It("Should not create a volume if an invalid volume name is passed", func() {
+			dataVolume := createValidDataVolume()
+			dataVolume.Name = "test"
+			_, err := c.CreateDataVolume(context.Background(), testNamespace, dataVolume)
+			Expect(err).To(Equal(ErrInvalidVolume))
+		})
+	})
+
 	Context("Snapshot class", func() {
 		BeforeEach(func() {
 			// Setup code before each test
@@ -118,23 +178,13 @@ var _ = Describe("Client", func() {
 	})
 
 	Context("Snapshot operators", func() {
-		createValidDataVolume := func(name string) *cdiv1.DataVolume {
-			return &cdiv1.DataVolume{
-				ObjectMeta: metav1.ObjectMeta{
-					Name:      name,
-					Namespace: testNamespace,
-				},
-				Spec: cdiv1.DataVolumeSpec{},
-			}
-		}
-
 		BeforeEach(func() {
 			// Setup code before each test
-			c = NewFakeCdiClient(NewFakeClient(), createValidDataVolume(testClaimName))
+			c = NewFakeCdiClient(NewFakeClient(), createValidDataVolume())
 		})
 
 		It("should return error if the volume snapshot class is not found", func() {
-			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", testClaimName, "non-existing-snapshot-class")
+			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", validDataVolume, "non-existing-snapshot-class")
 			Expect(err).To(HaveOccurred())
 			Expect(s).To(BeNil())
 			Expect(err.Error()).To(ContainSubstring("provided volume snapshot class cannot be matched with storage class"))
@@ -148,7 +198,7 @@ var _ = Describe("Client", func() {
 		})
 
 		It("should delete volumesnapshot if it exists and it valid", func() {
-			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", testClaimName, volumeSnapshotClassName)
+			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", validDataVolume, volumeSnapshotClassName)
 			Expect(err).ToNot(HaveOccurred())
 			Expect(s.Name).To(Equal("snap"))
 			err = c.DeleteVolumeSnapshot(context.TODO(), s.GetNamespace(), s.GetName())
@@ -161,16 +211,16 @@ var _ = Describe("Client", func() {
 		})
 
 		It("should return error if get volume returns an error", func() {
-			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", testClaimName, volumeSnapshotClassName)
+			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", validDataVolume, volumeSnapshotClassName)
 			Expect(err).ToNot(HaveOccurred())
 			Expect(s.Name).To(Equal("snap"))
-			c.infraLabelMap = map[string]string{"test": "test"}
+			c.infraLabelMap = map[string]string{"test": "test2"}
 			err = c.DeleteVolumeSnapshot(context.TODO(), s.GetNamespace(), s.GetName())
 			Expect(err).To(Equal(ErrInvalidSnapshot))
 		})
 
 		It("should properly list snapshots", func() {
-			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", testClaimName, volumeSnapshotClassName)
+			s, err := c.CreateVolumeSnapshot(context.TODO(), testNamespace, "snap", validDataVolume, volumeSnapshotClassName)
 			Expect(err).ToNot(HaveOccurred())
 			Expect(s.Name).To(Equal("snap"))
 			l, err := c.ListVolumeSnapshots(context.TODO(), testNamespace)
@@ -206,8 +256,15 @@ var _ = Describe("Client", func() {
 				util.StorageClassEnforcement{AllowList: []string{}, AllowDefault: true}, false),
 		)
 	})
+
 })
 
+func NewFakeCdiClient(c *client, objects ...runtime.Object) *client {
+	fakeCdiClient := cdicli.NewSimpleClientset(objects...)
+	c.cdiClient = fakeCdiClient
+	return c
+}
+
 func NewFakeClient() *client {
 	storageClass := createStorageClass(storageClassName, provisioner, false)
 	defaultStorageClass := createStorageClass(defaultStorageClassName, provisioner, true)
@@ -236,6 +293,8 @@ func NewFakeClient() *client {
 	result := &client{
 		kubernetesClient: fakeK8sClient,
 		snapClient:       fakeSnapClient,
+		infraLabelMap:    map[string]string{"test": "test"},
+		volumePrefix:     "pvc-",
 		storageClassEnforcement: util.StorageClassEnforcement{
 			AllowList:    []string{storageClassName},
 			AllowAll:     false,
@@ -245,12 +304,6 @@ func NewFakeClient() *client {
 	return result
 }
 
-func NewFakeCdiClient(c *client, objects ...runtime.Object) *client {
-	fakeCdiClient := cdicli.NewSimpleClientset(objects...)
-	c.cdiClient = fakeCdiClient
-	return c
-}
-
 func createVolumeSnapshotClass(name, provisioner string, isDefault bool) *snapshotv1.VolumeSnapshotClass {
 	res := &snapshotv1.VolumeSnapshotClass{
 		ObjectMeta: metav1.ObjectMeta{
@@ -282,6 +335,7 @@ func createPersistentVolumeClaim(name, volumeName, storageClassName string) *k8s
 		ObjectMeta: metav1.ObjectMeta{
 			Name:      name,
 			Namespace: testNamespace,
+			Labels:    map[string]string{"test": "test"},
 		},
 		Spec: k8sv1.PersistentVolumeClaimSpec{
 			StorageClassName: ptr.To[string](storageClassName),
@@ -304,3 +358,26 @@ func createStorageClass(name, provisioner string, isDefault bool) *storagev1.Sto
 	}
 	return res
 }
+
+func createDataVolume(name string, labels map[string]string) *cdiv1.DataVolume {
+	return &cdiv1.DataVolume{
+		ObjectMeta: metav1.ObjectMeta{
+			Name:      name,
+			Namespace: testNamespace,
+			Labels:    labels,
+		},
+		Spec: cdiv1.DataVolumeSpec{},
+	}
+}
+
+func createValidDataVolume() *cdiv1.DataVolume {
+	return createDataVolume(validDataVolume, map[string]string{"test": "test"})
+}
+
+func createNoLabelDataVolume() *cdiv1.DataVolume {
+	return createDataVolume(nolabelDataVolume, nil)
+}
+
+func createWrongPrefixDataVolume() *cdiv1.DataVolume {
+	return createDataVolume(testVolumeName, map[string]string{"test": "test"})
+}
diff --git a/pkg/service/controller.go b/pkg/service/controller.go
index 6f63b2c9..372c0c8d 100644
--- a/pkg/service/controller.go
+++ b/pkg/service/controller.go
@@ -256,6 +256,11 @@ func (c *ControllerService) ControllerPublishVolume(
 		return nil, err
 	}
 	dvName := req.GetVolumeId()
+	if _, err := c.virtClient.GetDataVolume(ctx, c.infraClusterNamespace, dvName); errors.IsNotFound(err) {
+		return nil, status.Errorf(codes.NotFound, "volume %s not found", req.GetVolumeId())
+	} else if err != nil {
+		return nil, err
+	}
 
 	klog.V(3).Infof("Attaching DataVolume %s to Node ID %s", dvName, req.NodeId)
 
